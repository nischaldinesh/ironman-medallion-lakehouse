{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1fa848-a059-470a-b43e-e99b16552496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9c0eba-bff0-4aa1-9478-37bbd2530ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"pipeline_config_json\", \"\", \"Pipeline Config JSON (from 01_config)\")\n",
    "dbutils.widgets.text(\"run_mode\", \"full\", \"Run Mode\")  # fallback only\n",
    "\n",
    "pipeline_config_json = dbutils.widgets.get(\"pipeline_config_json\").strip()\n",
    "\n",
    "if pipeline_config_json:\n",
    "    pipeline_config = json.loads(pipeline_config_json)\n",
    "\n",
    "    run_mode = pipeline_config.get(\"run_mode\", \"full\")\n",
    "    FULL_TABLE_NAME = pipeline_config[\"bronze_table\"]\n",
    "    VOLUME_PATH = pipeline_config[\"volume_path\"]\n",
    "    FILES_CONFIG = pipeline_config[\"files_to_process\"]\n",
    "\n",
    "    incr_cfg = pipeline_config.get(\"incremental\", {})\n",
    "    merge_key_cols = incr_cfg.get(\"merge_key_cols\", [\"row_key\"])\n",
    "\n",
    "else:\n",
    "    # Fallback\n",
    "    run_mode = dbutils.widgets.get(\"run_mode\")\n",
    "\n",
    "    CATALOG = \"ironman\"\n",
    "    BRONZE_SCHEMA = \"bronze\"\n",
    "    TABLE_NAME = \"ironman_results\"\n",
    "    FULL_TABLE_NAME = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "    VOLUME_PATH = \"/Volumes/ironman/default/landing\"\n",
    "\n",
    "    FILES_CONFIG = [\n",
    "        {\"filename\": \"2023_men.csv\", \"year\": 2023, \"gender\": \"M\"},\n",
    "        {\"filename\": \"2023_women.csv\", \"year\": 2023, \"gender\": \"F\"},\n",
    "        # {\"filename\": \"2024_men.csv\", \"year\": 2024, \"gender\": \"M\"},\n",
    "        # {\"filename\": \"2024_women.csv\", \"year\": 2024, \"gender\": \"F\"},\n",
    "        # {\"filename\": \"2025_men.csv\", \"year\": 2025, \"gender\": \"M\"},\n",
    "        # {\"filename\": \"2025_women.csv\", \"year\": 2025, \"gender\": \"F\"},\n",
    "    ]\n",
    "\n",
    "    merge_key_cols = [\"row_key\"]\n",
    "\n",
    "print(f\"Target: {FULL_TABLE_NAME}\")\n",
    "print(f\"Run Mode: {run_mode}\")\n",
    "print(f\"Files to process: {[f['filename'] for f in FILES_CONFIG]}\")\n",
    "print(f\"Merge keys: {merge_key_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507778c9-3bcd-421d-bd76-c226b76a4897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_config_json = dbutils.widgets.get(\"pipeline_config_json\").strip()\n",
    "print(\"pipeline_config_json length:\", len(pipeline_config_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a1bd17-4c8b-4ba8-b743-bb0ab583839e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_with_metadata(spark, file_path: str, year: int, gender: str):\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .csv(file_path)\n",
    "    )\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(col_name) == \"-\", None).otherwise(F.col(col_name))\n",
    "        )\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"year\", F.lit(year).cast(IntegerType()))\n",
    "        .withColumn(\"source_gender\", F.lit(gender).cast(StringType()))\n",
    "        .withColumn(\"source_file\", F.lit(file_path).cast(StringType()))\n",
    "        .withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"load_date\", F.current_date())\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7d6aed-0e16-485b-b8fd-b3f87ec28aec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "\n",
    "for config in FILES_CONFIG:\n",
    "    file_path = f\"{VOLUME_PATH}/year={config['year']}/{config['filename']}\"\n",
    "    df = read_csv_with_metadata(spark, file_path, config[\"year\"], config[\"gender\"])\n",
    "    row_count = df.count()\n",
    "    dataframes.append(df)\n",
    "    print(f\"Read {row_count:,} rows from {config['filename']}\")\n",
    "\n",
    "bronze_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    bronze_df = bronze_df.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "print(f\"\\nTotal rows: {bronze_df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1192212-67cd-49d4-85d0-5e37ab4ecc18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = bronze_df.withColumn(\n",
    "    \"athlete_name_clean\",\n",
    "    F.lower(F.regexp_replace(F.col(\"athlete_name\"), \"[^a-zA-Z0-9]\", \"\"))\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"year\", \"source_gender\", \"athlete_name_clean\").orderBy(\n",
    "    F.col(\"rank\").asc_nulls_last(),\n",
    "    F.col(\"bib\").asc_nulls_last()\n",
    ")\n",
    "bronze_df = bronze_df.withColumn(\"dup_rank\", F.row_number().over(window_spec))\n",
    "\n",
    "bronze_df = bronze_df.withColumn(\n",
    "    \"row_key\",\n",
    "    F.concat(\n",
    "        F.col(\"year\").cast(\"string\"),\n",
    "        F.lit(\"_\"),\n",
    "        F.col(\"source_gender\"),\n",
    "        F.lit(\"_\"),\n",
    "        F.col(\"athlete_name_clean\"),\n",
    "        F.lit(\"_\"),\n",
    "        F.col(\"dup_rank\").cast(\"string\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304f11e9-3b25-42e7-804a-426901c4c828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = bronze_df.drop(\"athlete_name_clean\", \"dup_rank\")\n",
    "\n",
    "dup_count = bronze_df.groupBy(\"row_key\").count().filter(F.col(\"count\") > 1).count()\n",
    "print(f\"Duplicate keys: {dup_count}\")\n",
    "\n",
    "print(\"Schema:\")\n",
    "bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6237bd58-442a-4c81-a687-f9c54a82ff0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_exists = spark.catalog.tableExists(FULL_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d546111-ed35-41f7-a1b5-31cc4472f1c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_condition = \" AND \".join([f\"target.{c} = source.{c}\" for c in merge_key_cols])\n",
    "\n",
    "if (not table_exists) or (run_mode == \"full\"):\n",
    "    print(f\"Writing full load to {FULL_TABLE_NAME}\")\n",
    "    (\n",
    "        bronze_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(FULL_TABLE_NAME)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Incremental merge (insert-only) into {FULL_TABLE_NAME}\")\n",
    "    delta_table = DeltaTable.forName(spark, FULL_TABLE_NAME)\n",
    "    (\n",
    "        delta_table.alias(\"target\")\n",
    "        .merge(bronze_df.alias(\"source\"), merge_condition)\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "print(\"Write complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df456587-3f0b-4f77-b0b5-460e63443504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.table(FULL_TABLE_NAME)\n",
    "\n",
    "print(f\"Table: {FULL_TABLE_NAME}\")\n",
    "print(f\"Rows: {result_df.count():,}\")\n",
    "\n",
    "display(\n",
    "    result_df\n",
    "    .groupBy(\"year\", \"source_gender\")\n",
    "    .count()\n",
    "    .orderBy(\"year\", \"source_gender\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"OPTIMIZE {FULL_TABLE_NAME}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BRONZE LAYER COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1115b70a-c907-408f-850d-1511855bc8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"SUCCESS\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_bronze",
   "widgets": {
    "pipeline_config_json": {
     "currentValue": "",
     "nuid": "eabc4042-e707-453f-b520-270275bf1053",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Pipeline Config JSON (from 01_config)",
      "name": "pipeline_config_json",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Pipeline Config JSON (from 01_config)",
      "name": "pipeline_config_json",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "run_mode": {
     "currentValue": "full",
     "nuid": "cd1236e5-277d-4458-a07d-8c69c973224a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "full",
      "label": "Run Mode",
      "name": "run_mode",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "full",
      "label": "Run Mode",
      "name": "run_mode",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}